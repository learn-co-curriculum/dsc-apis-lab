{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Yelp API - Lab\n", "\n", "\n", "## Introduction \n", "\n", "Now that we've seen how the Yelp API works and some basic Folium visualizations, it's time to put those skills to work in order to create a working map! Taking things a step further, you'll also independently explore how to perform pagination in order to retrieve a full results set from the Yelp API!\n", "\n", "## Objectives\n", "\n", "You will be able to: \n", "* Create HTTP requests to get data from Yelp API\n", "* Parse HTTP responses and perform data analysis on the data returned\n", "* Perform pagination to retrieve troves of data!\n", "* Create a simple geographical system on to view information about selected businesses, at a given location. \n", "\n", "## Problem Introduction\n", "\n", "You've now worked with some API calls, but we have yet to see how to retrieve a more complete dataset in a programmatic manner. Returning to the Yelp API, the [documentation](https://www.yelp.com/developers/documentation/v3/business_search) also provides us details regarding the API limits. These often include details about the number of requests a user is allowed to make within a specified time limit and the maximum number of results to be returned. In this case, we are told that any request has a maximum of 50 results per request and defaults to 20. Furthermore, any search will be limited to a total of 1000 results. To retrieve all 1000 of these results, we would have to page through the results piece by piece, retrieving 50 at a time. Processes such as these are often referred to as pagination.\n", "\n", "In this lab, you will define a search and then paginate over the results to retrieve all of the results. You'll then parse these responses as a DataFrame (for further exploration) and create a map using Folium to visualize the results geographically.\n", "\n", "## Part I - Make the Initial Request\n", "\n", "Start by making an initial request to the Yelp API. Your search must include at least 2 parameters: **term** and **location**. For example, you might search for pizza restaurants in NYC. The term and location is up to you, but make the request below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# IMPORTANT: THIS SOLUTION WILL NOT RUN UNLESSS YOU FILL IN THE CLIENT_ID AND API_KEY!\n", "\n", "import requests\n", "import pandas as pd\n", "\n", "client_id = \n", "api_key = \n", "\n", "term = 'pizza'\n", "location = 'New York NY'\n", "\n", "url = 'https://api.yelp.com/v3/businesses/search'\n", "\n", "headers = {\n", "        'Authorization': 'Bearer {}'.format(api_key),\n", "    }\n", "\n", "url_params = {\n", "                'term': term.replace(' ', '+'),\n", "                'location': location.replace(' ', '+'),\n", "            }\n", "response = requests.get(url, headers=headers, params=url_params) #Your code here\n", "print(response)\n", "print(type(response.text))\n", "print(response.text[:1000])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["len(response.json()['businesses'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["response.json()['total']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Pagination\n", "\n", "Now that you have an initial response, you can examine the contents of the json container. For example, you might start with ```response.json().keys()```. Here, you'll see a key for `'total'`, which tells you the full number of matching results given your query parameters. Write a loop (or ideally a function) which then makes successive API calls using the offset parameter to retrieve all of the results (or 5000 for a particularly large result set) for the original query. As you do this, be mindful of how you store the data. Your final goal will be to reformat the data concerning the businesses themselves into a pandas DataFrame from the json objects.\n", "\n", "**Note: be mindful of the API rate limits. You can only make 5000 requests per day, and are also can make requests too fast. Start prototyping small before running a loop that could be faulty. You can also use time.sleep(n) to add delays. For more details see https://www.yelp.com/developers/documentation/v3/rate_limiting.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import time\n", "\n", "def yelp_call(url_params, api_key):\n", "    url = 'https://api.yelp.com/v3/businesses/search'\n", "    headers = {'Authorization': 'Bearer {}'.format(api_key)}\n", "    response = requests.get(url, headers=headers, params=url_params)\n", "    \n", "    df = pd.DataFrame(response.json()['businesses'])\n", "    return df\n", "\n", "def all_results(url_params, api_key):\n", "    num = response.json()['total']\n", "    print('{} total matches found.'.format(num))\n", "    cur = 0\n", "    dfs = []\n", "    while cur < num and cur < 1000:\n", "        url_params['offset'] = cur\n", "        dfs.append(yelp_call(url_params, api_key))\n", "        time.sleep(1) #Wait a second\n", "        cur += 50\n", "    df = pd.concat(dfs, ignore_index=True)\n", "    return df\n", "\n", "term = 'pizza'\n", "location = 'Astoria NY'\n", "url_params = {  'term': term.replace(' ', '+'),\n", "                'location': location.replace(' ', '+'),\n", "                'limit' : 50\n", "             }\n", "df = all_results(url_params, api_key)\n", "print(len(df))\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exploratory Analysis\n", "\n", "Take the restaurants from the previous question and do an initial exploratory analysis. At minimum, this should include looking at the distribution of features such as price, rating and number of reviews as well as the relations between these dimensions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "df.price = df.price.fillna(value=0)\n", "price_dict = {\"$\": 1, \"$$\":2, \"$$$\": 3, \"$$$$\":4}\n", "df.price = df.price.map(price_dict)\n", "\n", "pd.plotting.scatter_matrix(df[['price', 'rating', 'review_count']])\n", "Out[5]:\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Mapping\n", "\n", "Look at the initial Yelp example and try and make a map using Folium of the restaurants you retrieved. Be sure to also add popups to the markers giving some basic information such as name, rating and price."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import folium\n", "\n", "lat_long = df['coordinates'].iloc[0]\n", "lat = lat_long['latitude']\n", "long = lat_long['longitude']\n", "yelp_map = folium.Map([lat, long])\n", "\n", "for row in df.index:\n", "    try:\n", "        lat_long = df['coordinates'][row]\n", "        lat = lat_long['latitude']\n", "        long = lat_long['longitude']\n", "        name = df['name'][row]\n", "        rating = df['rating'][row]\n", "        price = df['price'][row]\n", "        details = \"{}\\nPrice: {} Rating:{}\".format(name,str(price),str(rating))\n", "        popup = folium.Popup(details, parse_html=True)\n", "        marker = folium.Marker([lat, long], popup=popup)\n", "        marker.add_to(yelp_map)\n", "    except:\n", "        print('Hit error on row: {}'.format(row))\n", "yelp_map"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "Nice work! In this lab, you synthesized your skills for the day, making multiple API calls to Yelp in order to paginate through a results set, performing some basic exploratory analysis and then creating a nice map visual to display the results! Well done!"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}}, "nbformat": 4, "nbformat_minor": 2}